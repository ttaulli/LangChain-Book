{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97df42a",
   "metadata": {},
   "source": [
    "# Multi-Model Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ce396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec52a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import HuggingFaceHub\n",
    "from getpass import getpass\n",
    "from langchain.schema import AIMessage, HumanMessage,SystemMessage\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be067b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20efc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7674ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dca97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604eb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_process(question):\n",
    "    openai_llm = ChatOpenAI(openai_api_key=openai.api_key, model_name=\"gpt-3.5-turbo\", max_tokens=100)\n",
    "    reply = openai_llm([\n",
    "        SystemMessage(content='You are a helpful assistant.'),\n",
    "        HumanMessage(content=question)\n",
    "    ])\n",
    "    \n",
    "    return reply.content\n",
    "\n",
    "def flan_process(question):\n",
    "    flan_llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xxl\", model_kwargs={\"max_length\": 100}\n",
    "    )  \n",
    "    template = \"{question}\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=flan_llm)\n",
    "\n",
    "    return llm_chain.run(question)\n",
    "def dolly_process(question):\n",
    "    dolly_llm = HuggingFaceHub(\n",
    "    repo_id=\"databricks/dolly-v2-3b\", model_kwargs={\"max_length\": 100}\n",
    "    )  \n",
    "    template = \"{question}\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=dolly_llm)\n",
    "\n",
    "    return llm_chain.run(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4edbd7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a prompt: What is the transformer model?\n",
      "----------\n",
      "OpenAI reply: The transformer model is a type of deep learning model that was introduced in a 2017 paper titled \"Attention is All You Need\" by Vaswani et al. It has achieved state-of-the-art performance in various natural language processing tasks, such as machine translation, language understanding, and text generation.\n",
      "\n",
      "The transformer model relies heavily on self-attention mechanisms, which allow the model to focus on different parts of the input sequence to better capture dependencies between words. Unlike traditional recurrent neural networks (RNN\n",
      "----------\n",
      "Flan reply: a reversible electrical circuit\n",
      "----------\n",
      "Dolly reply: \n",
      "\n",
      "The transformer model is a neural network architecture based on the attention mechanism. It was introduced in the paper \"Attention is all you need\" by S. G. Seitz, N. Jaitly, A. Karpathy, A. Khosla, and A. Senior. The transformer model is composed of an encoder, a decoder, and a attention mechanism between the encoder and the decoder. The attention mechanism is responsible for finding the most relevant parts\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Please enter a prompt: \")\n",
    "openai_reply = openai_process(question)\n",
    "print(\"-\"*10)\n",
    "print(f\"OpenAI reply: {openai_reply}\")\n",
    "print(\"-\"*10)\n",
    "flan_reply = flan_process(question)\n",
    "print(f\"Flan reply: {flan_reply}\")\n",
    "print(\"-\"*10)\n",
    "dolly_reply = dolly_process(question)\n",
    "print(f\"Dolly reply: {dolly_reply}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5925d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
